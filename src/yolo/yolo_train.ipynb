{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11s summary: 319 layers, 9,428,179 parameters, 0 gradients, 21.5 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(319, 9428179, 0, 21.548492800000002)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# start model \"../../models/yolo11s.pt\"\n",
    "# 1. train run \"../../runs/detect/train/weights/last.pt\"\n",
    "# 2. train run \"../../runs/detect/train2/weights/last.pt\" -> worse results than first model\n",
    "model = YOLO(\"../../runs/detect/train/weights/best.pt\")\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune on your dataset\n",
    "results = model.train(\n",
    "    data=\"data.yaml\",\n",
    "    epochs=10,\n",
    "    batch=32,\n",
    "    imgsz=640, # Rescales images to this size\n",
    "    device=\"cuda\",\n",
    "    # freeze = 300, # for finetuning instead of retraining https://github.com/ultralytics/ultralytics/issues/6184 -> I will retrain\n",
    "    # resume=True, # to continue training\n",
    "    # fraction = 0.3 # only use this amount of the test data  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.70  Python-3.12.6 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Zeug\\Unizeug\\Master_DataScience\\3.Semester\\Learning from Images\\Project\\data\\merged\\labels\\val.cache... 1132 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1132/1132 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1132/1132 [00:20<00:00, 56.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1132       1635      0.852      0.791      0.846      0.495\n",
      "Speed: 0.4ms preprocess, 12.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1md:\\Zeug\\Unizeug\\Master_DataScience\\3.Semester\\Learning from Images\\Project\\runs\\detect\\val8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val(conf=0.25, iou=0.5, half=False, batch=1, rect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Box(P)': 0.8171428571428572, 'Box(R)': 0.7871559633027523, 'mAP50': tensor(0.9467), 'mAP50-95': np.float64(0.6645460531408871), 'Confusion Matrix': {'TP': 1287, 'FP': 288, 'FN': 348, 'TN': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 d:\\Zeug\\Unizeug\\Master_DataScience\\3.Semester\\Learning from Images\\Project\\src\\yolo\\..\\..\\data\\img.jpg: 480x640 1 license_plate, 44.0ms\n",
      "Speed: 3.1ms preprocess, 44.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "results = model(\"../../data/img.jpg\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.70  Python-3.12.6 torch-2.6.0+cu126 CPU (Intel Core(TM) i7-8700K 3.70GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '..\\..\\runs\\detect\\train\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.6s, saved as '..\\..\\runs\\detect\\train\\weights\\best.onnx' (36.2 MB)\n",
      "\n",
      "Export complete (4.2s)\n",
      "Results saved to \u001b[1mD:\\Zeug\\Unizeug\\Master_DataScience\\3.Semester\\Learning from Images\\Project\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=..\\..\\runs\\detect\\train\\weights\\best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=..\\..\\runs\\detect\\train\\weights\\best.onnx imgsz=640 data=data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "path = model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torchmetrics.classification import AveragePrecision\n",
    "\n",
    "\n",
    "def iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area_box1 + area_box2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def evaluate_model(model, images, bboxes, predict_function, conf_threshold = 0.01, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using given images and annotations.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of images.\n",
    "        annotations (list): List of ground truth bounding boxes.\n",
    "        predict_function (function): Function that takes an image and returns (confidences, predicted_boxes).\n",
    "        iou_threshold (float): IoU threshold to determine TP vs FP.\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    predicted_confidences = []\n",
    "    predicted_true_labels = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    for img, ground_truths in zip(images, bboxes):\n",
    "        confidences, predicted_boxes = predict_function(model, img)\n",
    "        # TODO sort confidences and predicted boxes by confidence scores\n",
    "        matched = set()\n",
    "        \n",
    "        for conf, pred_box in zip(confidences, predicted_boxes):\n",
    "            if conf < conf_threshold:\n",
    "                break\n",
    "            \n",
    "            iou_max = 0\n",
    "            matched_gt = None\n",
    "            \n",
    "            for i, gt_box in enumerate(ground_truths):\n",
    "                iou_score = iou(pred_box, gt_box)\n",
    "                if iou_score > iou_max:\n",
    "                    iou_max = iou_score\n",
    "                    matched_gt = i\n",
    "\n",
    "            \n",
    "            predicted_confidences.append(conf)\n",
    "            predicted_true_labels.append(1 if iou_max >= iou_threshold and matched_gt not in matched else 0)\n",
    "            iou_scores.append(iou_max)\n",
    "            \n",
    "            if predicted_true_labels[-1] == 1:\n",
    "                # predicted: license plate, true: license plate\n",
    "                tp += 1\n",
    "                matched.add(matched_gt)\n",
    "            else:\n",
    "                # predicted: license plate, true: background\n",
    "                fp += 1\n",
    "        \n",
    "        # predicted: background, true: license plate (license plates that were not predicted)\n",
    "        fn += len(ground_truths) - len(matched)\n",
    "        # tn: predicted: background, true: background (we dont predict the background, cant say number of background bounding boxes)\n",
    "\n",
    "    confusion_matrix = {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": None}\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # TODO Doesnt make sense to have mAP in the same function as precision and recall where iou is an argument\n",
    "    # https://www.v7labs.com/blog/mean-average-precision\n",
    "    # Blog Describes AP Wrong: https://www.reddit.com/r/computervision/comments/162ss9x/trouble_understanding_map50_metric/\n",
    "    # TODO Map = go over different confidences and calculate precision, take mean precision\n",
    "    # TODO below is wrong just write own python method\n",
    "    ap = AveragePrecision(task=\"binary\")\n",
    "    mAP50 = ap(tensor(predicted_confidences), tensor(predicted_true_labels))\n",
    "    # TODO mean average precision for different IOU thresholds, I need to calculate precision for different IoU Thresholds 0.5, 0.55, 0.6,..., 0.9\n",
    "    mAP50_95 = np.mean(iou_scores) \n",
    "    \n",
    "    return {\n",
    "        \"Box(P)\": precision,\n",
    "        \"Box(R)\": recall,\n",
    "        \"mAP50\": mAP50,\n",
    "        \"mAP50-95\": mAP50_95,\n",
    "        \"Confusion Matrix\": confusion_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict(model, img):\n",
    "    # default arguments which overwrite the evaluation function\n",
    "    # https://docs.ultralytics.com/modes/predict/#inference-arguments\n",
    "    results = model(img, verbose=False)\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            confidences.append(box.conf.item())\n",
    "            boxes.append(box.xyxy[0].tolist())\n",
    "    \n",
    "    return confidences, boxes\n",
    "\n",
    "def yolo_to_bbox(annotation_path, img_width, img_height):\n",
    "    bboxes = []\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # I have only one class and ignore the class id\n",
    "            _, x_center, y_center, width, height = map(float, parts)  \n",
    "            x_min = int((x_center - width / 2) * img_width)\n",
    "            y_min = int((y_center - height / 2) * img_height)\n",
    "            x_max = int((x_center + width / 2) * img_width)\n",
    "            y_max = int((y_center + height / 2) * img_height)\n",
    "            \n",
    "            bboxes.append([x_min, y_min, x_max, y_max])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(\"../../data/merged/images/val/img_dataset_l25471.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.13289062499999998 0.8077083333333333 0.02062499999999999 0.01250000000000003\n",
      "0 0.7575390625 0.7143753333333334 0.01937499999999992 0.011665999999999954\n",
      "0 0.852421875 0.8362496666666667 0.028750000000000053 0.020833999999999946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/merged/labels/val/img_dataset_l25471.txt\", \"r\") as s:\n",
    "    print(s.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6273])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_iou(tensor([0.1328, 0.8077, 0.0206, 0.0125]), tensor([0.1336, 0.8072, 0.0240, 0.0171]), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0., 0., 0., 0.], device='cuda:0')\n",
       "conf: tensor([0.6007, 0.5381, 0.5187, 0.2753], device='cuda:0')\n",
       "data: tensor([[8.6098e+02, 6.3628e+02, 8.8565e+02, 6.4966e+02, 6.0071e-01, 0.0000e+00],\n",
       "        [1.2452e+02, 6.1338e+02, 1.4905e+02, 6.2650e+02, 5.3813e-01, 0.0000e+00],\n",
       "        [2.4927e+00, 7.2909e+02, 3.4923e+01, 7.4470e+02, 5.1873e-01, 0.0000e+00],\n",
       "        [1.6709e+02, 5.9117e+02, 1.9439e+02, 6.0522e+02, 2.7529e-01, 0.0000e+00]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (768, 1024)\n",
       "shape: torch.Size([4, 6])\n",
       "xywh: tensor([[873.3170, 642.9738,  24.6746,  13.3813],\n",
       "        [136.7847, 619.9441,  24.5333,  13.1199],\n",
       "        [ 18.7079, 736.8942,  32.4305,  15.6099],\n",
       "        [180.7380, 598.1912,  27.3021,  14.0504]], device='cuda:0')\n",
       "xywhn: tensor([[0.8528, 0.8372, 0.0241, 0.0174],\n",
       "        [0.1336, 0.8072, 0.0240, 0.0171],\n",
       "        [0.0183, 0.9595, 0.0317, 0.0203],\n",
       "        [0.1765, 0.7789, 0.0267, 0.0183]], device='cuda:0')\n",
       "xyxy: tensor([[860.9797, 636.2831, 885.6543, 649.6645],\n",
       "        [124.5181, 613.3841, 149.0513, 626.5040],\n",
       "        [  2.4927, 729.0893,  34.9232, 744.6992],\n",
       "        [167.0870, 591.1660, 194.3891, 605.2164]], device='cuda:0')\n",
       "xyxyn: tensor([[0.8408, 0.8285, 0.8649, 0.8459],\n",
       "        [0.1216, 0.7987, 0.1456, 0.8158],\n",
       "        [0.0024, 0.9493, 0.0341, 0.9697],\n",
       "        [0.1632, 0.7697, 0.1898, 0.7880]], device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "image_dir = \"../../data/merged/images/val\"\n",
    "annotation_dir = \"../../data/merged/labels/val\"\n",
    "\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)]\n",
    "annotation_paths = [os.path.join(annotation_dir, f) for f in os.listdir(annotation_dir)]\n",
    "assert len(image_paths) == len(annotation_paths)\n",
    "\n",
    "#image_paths = [\"../../data/merged/images/test/img_dataset_s431.jpg\", \"../../data/merged/images/test/img_dataset_s432.jpg\"]\n",
    "#annotation_paths = [\"../../data/merged/labels/test/img_dataset_s431.txt\", \"../../data/merged/labels/test/img_dataset_s432.txt\"]\n",
    "\n",
    "\n",
    "all_bboxes = []\n",
    "for annotation_path, img_path in zip(annotation_paths, image_paths):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_height, img_width = img.shape[0], img.shape[1]\n",
    "    # Convert YOLO annotations\n",
    "    bboxes = yolo_to_bbox(annotation_path, img_width, img_height)\n",
    "    all_bboxes.append(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Box(P)': 0.817142857090975, 'Box(R)': 0.7871559632546081, 'mAP50': tensor(0.9463), 'mAP50-95': np.float64(0.6645460531408871), 'Confusion Matrix': {'TP': 1287, 'FP': 288, 'FN': 348, 'TN': None}}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, image_paths, all_bboxes, yolo_predict)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
