{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.conversion_helpers import yolo_to_bbox\n",
    "from utils.evaluation_helper import ObjectDetectionEvaluator\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# with help of this tutorial: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "        annotation_path = os.path.join(self.annotations_dir, image_name.replace('.jpg', '.txt'))\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        H, W, _ = image.shape\n",
    "        boxes = yolo_to_bbox(annotation_path, W, H)\n",
    "        \n",
    "        #image, boxes = self.resize_and_pad(image, boxes, (640, 640)) -> gave me worse results than internal scaling\n",
    "        image = transforms.ToTensor()(image)\n",
    "        # Handle empty bounding boxes\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)  # No bounding boxes\n",
    "            labels = torch.zeros(1, dtype=torch.int64)  # No labels\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones(len(boxes), dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def resize_and_pad(self, image, boxes, target_size):\n",
    "        # gave me worse results than internal scaling\n",
    "        h, w, _ = image.shape\n",
    "        scale = min(target_size[0] / w, target_size[1] / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        \n",
    "        resized_image = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        pad_x = (target_size[0] - new_w) // 2\n",
    "        pad_y = (target_size[1] - new_h) // 2\n",
    "        \n",
    "        padded_image = np.full((target_size[1], target_size[0], 3), 0, dtype=np.uint8)\n",
    "        padded_image[pad_y:pad_y + new_h, pad_x:pad_x + new_w] = resized_image\n",
    "        \n",
    "        adjusted_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            x_min = x_min * scale + pad_x\n",
    "            y_min = y_min * scale + pad_y\n",
    "            x_max = x_max * scale + pad_x\n",
    "            y_max = y_max * scale + pad_y\n",
    "            adjusted_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        return transforms.ToTensor()(padded_image), adjusted_boxes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None):\n",
    "    model.train()\n",
    "    \n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "    \n",
    "    total_loss = 0\n",
    "    for images, targets in tqdm(data_loader, total=len(data_loader), desc=\"Processing batches\"):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Enable mixed precision -> helps with GPU Memory\n",
    "        with autocast(\"cuda\", enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def fasterrcnn_predict(model, img, conf_threshold = 0.001):\n",
    "    model.eval()\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(img)\n",
    "\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for output in outputs:\n",
    "        if \"scores\" in output and \"boxes\" in output:\n",
    "            scores = output[\"scores\"].cpu().numpy()\n",
    "            bboxes = output[\"boxes\"].cpu().numpy()\n",
    "            \n",
    "            valid_indices = scores > conf_threshold\n",
    "            scores = scores[valid_indices]\n",
    "            bboxes = bboxes[valid_indices]\n",
    "    \n",
    "            confidences.extend(scores.tolist())\n",
    "            boxes.extend(bboxes.tolist())\n",
    "    \n",
    "    return confidences, boxes\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_imges = []\n",
    "    all_bboxes = []\n",
    "    for images, targets in data_loader:\n",
    "        all_imges.extend([img.to(device) for img in images])\n",
    "        all_bboxes.extend([target[\"boxes\"].cpu().numpy().tolist() for target in targets])\n",
    "       \n",
    "    evaluator = ObjectDetectionEvaluator(model, all_imges, all_bboxes, fasterrcnn_predict)\n",
    "    metric_summary = evaluator.get_metric_summary(verbose=False)\n",
    "    \n",
    "    return metric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_subset(dataset, fraction=0.2):\n",
    "    subset_size = int(len(dataset) * fraction)\n",
    "    random.seed(1234)\n",
    "    indices = random.sample(range(len(dataset)), subset_size)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Train Dataset\n",
    "images_dir = \"../../data/merged/images/train\"\n",
    "annotations_dir = \"../../data/merged/labels/train\"\n",
    "train_dataset = LicensePlateDataset(images_dir, annotations_dir)\n",
    "#train_subset = get_subset(train_dataset, fraction=0.03) # for testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Validation Dataset\n",
    "validation_images_dir = \"../../data/merged/images/val\"\n",
    "validation_annotation_dir = \"../../data/merged/labels/val\"\n",
    "val_dataset = LicensePlateDataset(validation_images_dir, validation_annotation_dir)\n",
    "#val_subset = get_subset(val_dataset, fraction=0.2) # for testing\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model fasterrcnn_resnet50_fpn_v2 to cuda\n"
     ]
    }
   ],
   "source": [
    "# use pretrained weights\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Loaded Model fasterrcnn_resnet50_fpn_v2 to {device}\")\n",
    "\n",
    "# scaler helps me with fitting larger batches in the GPU memory, lowers the precision of floats\n",
    "scaler = GradScaler(\"cuda\") \n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Checkpoint! Start epoch = 0\n"
     ]
    }
   ],
   "source": [
    "best_mAP50 = 0.0\n",
    "best_mAP50_95 = 0.0\n",
    "stopping_counter = 0\n",
    "patience = 5\n",
    "num_epochs = 10\n",
    "start_epoch = 0\n",
    "\n",
    "continue_training = True\n",
    "if continue_training:\n",
    "    checkpoint = torch.load(\"../../models/best_fastrcnn.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Loaded Checkpoint! Start epoch = {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 5167/5167 [1:32:34<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 1132/1132 [28:28<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation: mAP50 = 0.9146, mAP50-95 = 0.6440\n",
      "Model saved at epoch 0.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, scaler)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}\")\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    metrics = evaluate(model, val_loader, device=device) \n",
    "    mAP50 = metrics['mAP50']\n",
    "    mAP50_95 = metrics['mAP50-95']\n",
    "    print(f\"Epoch {epoch} Validation: mAP50 = {mAP50:.4f}, mAP50-95 = {mAP50_95:.4f}\")\n",
    "    if mAP50 > best_mAP50 or mAP50_95 > best_mAP50_95:\n",
    "        best_mAP50 = max(mAP50, best_mAP50)\n",
    "        best_mAP50_95 = max(mAP50_95, best_mAP50_95)\n",
    "        stopping_counter = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"lr_scheduler_state_dict\": lr_scheduler.state_dict(),\n",
    "        }, f\"../../models/best_fastrcnn.pth\")\n",
    "        print(f\"Model saved at epoch {epoch}.\")\n",
    "    else:\n",
    "        stopping_counter += 1\n",
    "    \n",
    "    if stopping_counter == patience:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
